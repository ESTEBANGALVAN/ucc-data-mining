{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba8728e-0420-4992-ae3d-e4cfe036229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.3 pandas-2.2.3 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e53e09-008c-46b1-8f7b-8725d7e9924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a101c9e-1155-4bed-b6f2-72fbcde82bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m627.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bb136f-599e-4b87-bc08-2f1888a9a648",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.4 GiB for an array with shape (209572, 104674) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCase Number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFBI Code\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX Coordinate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY Coordinate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Codificar las variables categóricas\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Dividir los datos en características (X) y etiquetas (y)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrimary Type\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:214\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, pre, sep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[1;32m    224\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:353\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     dummy_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m--> 353\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m dummy_mat[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(codes)), codes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 20.4 GiB for an array with shape (209572, 104674) and data type bool"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "# Codificar las variables categóricas\n",
    "data = pd.get_dummies(data)\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "y = data['Primary Type']\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b282bdbd-58a5-4c53-aac9-884e2aed453b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.4 GiB for an array with shape (209572, 104674) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     data[col] \u001b[38;5;241m=\u001b[39m data[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Codificar las variables categóricas usando 'categorical' dtype\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Dividir los datos en características (X) y etiquetas (y)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrimary Type\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:214\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, pre, sep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[1;32m    224\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:353\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     dummy_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m--> 353\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m dummy_mat[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(codes)), codes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 20.4 GiB for an array with shape (209572, 104674) and data type bool"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Convertir algunas columnas categóricas a tipo 'category'\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# Codificar las variables categóricas usando 'categorical' dtype\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "y = data['Primary Type']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b74f4d0-978e-4e98-9e0c-6219f8c24e1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Primary Type'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Convertir el DataFrame disperso a una matriz dispersa CSR (Compressed Sparse Row)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m X_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrimary Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrimary Type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjuntos de entrenamiento y prueba\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Primary Type'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Convertir las columnas categóricas a tipo 'category'\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# Codificar las variables categóricas usando matrices dispersas\n",
    "data = pd.get_dummies(data, sparse=True)\n",
    "\n",
    "# Convertir el DataFrame disperso a una matriz dispersa CSR (Compressed Sparse Row)\n",
    "X_sparse = csr_matrix(data.drop(columns=['Primary Type']).values)\n",
    "y = data['Primary Type'].values\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d98895e-19c9-4333-bc00-290ca0e6aa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/2939418264.py:28: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  X_sparse = csr_matrix(data.values)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 208. GiB for an array with shape (133307, 209572) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Convertir el DataFrame disperso a una matriz dispersa CSR (Compressed Sparse Row)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m X_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjuntos de entrenamiento y prueba\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_sparse, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[1;32m  12594\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[1;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1727\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 1727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 208. GiB for an array with shape (133307, 209572) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar la columna 'Primary Type' antes de aplicar get_dummies\n",
    "y = data['Primary Type']\n",
    "data = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Convertir las columnas categóricas a tipo 'category'\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# Codificar las variables categóricas usando matrices dispersas\n",
    "data = pd.get_dummies(data, sparse=True)\n",
    "\n",
    "# Convertir el DataFrame disperso a una matriz dispersa CSR (Compressed Sparse Row)\n",
    "X_sparse = csr_matrix(data.values)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae823fcb-fbea-43f3-8556-633ec8e17a0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Codificar las variables categóricas usando OneHotEncoder con matrices dispersas\u001b[39;00m\n\u001b[1;32m     21\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 22\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m encoded_categorical_data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[categorical_columns])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Reemplazar las columnas categóricas originales por las codificadas\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar la columna 'Primary Type' antes de aplicar la codificación\n",
    "y = data['Primary Type']\n",
    "data = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Codificar las variables categóricas usando OneHotEncoder con matrices dispersas\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "encoder = OneHotEncoder(sparse=True)\n",
    "encoded_categorical_data = encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Reemplazar las columnas categóricas originales por las codificadas\n",
    "data = data.drop(columns=categorical_columns)\n",
    "X = hstack([data.values, encoded_categorical_data])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d489f4-cdf7-4b72-9822-e86394046340",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('O'), dtype('float64'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Reemplazar las columnas categóricas originales por las codificadas\u001b[39;00m\n\u001b[1;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcategorical_columns)\n\u001b[0;32m---> 27\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_categorical_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjuntos de entrenamiento y prueba\u001b[39;00m\n\u001b[1;32m     30\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/sparse/_construct.py:742\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _block([blocks], \u001b[38;5;28mformat\u001b[39m, dtype)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_spmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/sparse/_construct.py:971\u001b[0m, in \u001b[0;36m_block\u001b[0;34m(blocks, format, dtype, return_spmatrix)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     all_dtypes \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks[block_mask]]\n\u001b[0;32m--> 971\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mupcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_dtypes\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m all_dtypes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    973\u001b[0m row_offsets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mcumsum(brow_lengths))\n\u001b[1;32m    974\u001b[0m col_offsets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mcumsum(bcol_lengths))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/sparse/_sputils.py:55\u001b[0m, in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     52\u001b[0m         _upcast_memo[\u001b[38;5;28mhash\u001b[39m(args)] \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno supported conversion for types: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'), dtype('float64'))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar la columna 'Primary Type' antes de aplicar la codificación\n",
    "y = data['Primary Type']\n",
    "data = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Codificar las variables categóricas usando OneHotEncoder con matrices dispersas\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "encoder = OneHotEncoder(sparse_output=True)\n",
    "encoded_categorical_data = encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Reemplazar las columnas categóricas originales por las codificadas\n",
    "data = data.drop(columns=categorical_columns)\n",
    "X = hstack([data.values, encoded_categorical_data])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b0c4b-4647-4b8b-b18d-0a355d53ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar la columna 'Primary Type' antes de aplicar la codificación\n",
    "y = data['Primary Type']\n",
    "data = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Codificar las variables categóricas usando OneHotEncoder con matrices dispersas\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "encoder = OneHotEncoder(sparse_output=True)\n",
    "encoded_categorical_data = encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Convertir las columnas restantes a float\n",
    "non_categorical_data = data.drop(columns=categorical_columns).astype(float)\n",
    "\n",
    "# Convertir el DataFrame a una matriz dispersa CSR (Compressed Sparse Row)\n",
    "non_categorical_data_sparse = csr_matrix(non_categorical_data.values)\n",
    "\n",
    "# Combinar las matrices dispersas\n",
    "X_sparse = hstack([non_categorical_data_sparse, encoded_categorical_data])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bad2c50-24f5-43a5-859a-ed731cdad28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9948705713944889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['crime_prediction_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "# Codificar las variables categóricas con LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "   le = LabelEncoder()\n",
    "   data[column] = le.fit_transform(data[column])\n",
    "   label_encoders[column] = le\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "y = data['Primary Type']\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a88d33-e58c-4b68-b0da-8c8c4df244bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 31 (1364592156.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 32\u001b[0;36m\u001b[0m\n\u001b[0;31m    input_data[column] = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 31\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    " DATE_OF_OCCURRENCE: str\n",
    " BLOCK: str\n",
    " IUCR: str\n",
    " LOCATION_DESCRIPTION: str\n",
    " ARREST: str\n",
    " DOMESTIC: str\n",
    " BEAT: int\n",
    " WARD: int\n",
    "class CrimePredictionOutput(BaseModel):\n",
    " PRIMARY_DESCRIPTION: str\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    " # Convertir los datos de entrada en un DataFrame\n",
    " input_data = pd.DataFrame([data.dict()])\n",
    " # Preprocesar los datos de entrada\n",
    " input_data = pd.get_dummies(input_data)\n",
    " # Asegurarse de que todas las columnas necesarias estén presentes\n",
    " required_columns = set(X.columns)\n",
    " missing_columns = required_columns - set(input_data.columns)\n",
    " for column in missing_columns:\n",
    " input_data[column] = 0\n",
    " # Hacer la predicción\n",
    " prediction = model.predict(input_data)\n",
    " # Obtener la descripción primaria del crimen predicho\n",
    " primary_description = prediction[0]\n",
    " return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a479ab-e6bd-42ba-8d79-a9d2546a6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "   Date: str\n",
    "   Block: str\n",
    "   IUCR: str\n",
    "   Location: str\n",
    "   Arrest: str\n",
    "   Domestic: str\n",
    "   Beat: int\n",
    "   Ward: int\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "   Primary_Type: str\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/', response_model=CrimePredictionOutput)\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "   # Convertir los datos de entrada en un DataFrame\n",
    "   input_data = pd.DataFrame([data.dict()])\n",
    "   # Preprocesar los datos de entrada\n",
    "   input_data = pd.get_dummies(input_data)\n",
    "   # Simulación de las columnas X del entrenamiento para obtener required_columns\n",
    "   X = pd.DataFrame(columns=['Date', 'Block', 'IUCR', 'Location', 'Arrest', 'Domestic', 'Beat', 'Ward'])\n",
    "   required_columns = set(X.columns)\n",
    "   missing_columns = required_columns - set(input_data.columns)\n",
    "   for column in missing_columns:\n",
    "       input_data[column] = 0\n",
    "   # Asegurarse de que las columnas estén en el orden correcto\n",
    "   input_data = input_data[X.columns]\n",
    "   # Hacer la predicción\n",
    "   prediction = model.predict(input_data)\n",
    "   # Obtener la descripción primaria del crimen predicho\n",
    "   Description = prediction[0]\n",
    "   return CrimePredictionOutput(Primary_Type=Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdf0fd6-ad98-461b-9ed8-715e9b05583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from fastapi) (4.8.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from starlette<0.42.0,>=0.40.0->fastapi) (4.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m656.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m830.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, annotated-types, starlette, pydantic, fastapi\n",
      "Successfully installed annotated-types-0.7.0 fastapi-0.115.4 pydantic-2.9.2 pydantic-core-2.23.4 starlette-0.41.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f5bf5f-3d97-4413-854d-9ed028da0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting click>=7.0 (from uvicorn)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting h11>=0.8 (from uvicorn)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m47.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m187.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m53.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h11, click, uvicorn\n",
      "Successfully installed click-8.1.7 h11-0.14.0 uvicorn-0.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55a1d6f-41ae-454e-8eae-06500046c248",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3014011554.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"Date\": \"08/31/2023 07:00:00 PM\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    " 'http://localhost:8000/predict/' \\\n",
    " -H 'accept: application/json' \\\n",
    " -H 'Content-Type: application/json' \\\n",
    " -d '{\n",
    " \"Date\": \"08/31/2023 07:00:00 PM\",\n",
    " \"Block\": \"042XX W MARQUETTE RD\",\n",
    " \"IUCR\": \"0498\",\n",
    " \"Location Description\": \"APARTMENT\",\n",
    " \"Arrest\": \"Y\",\n",
    " \"Domestic\": \"Y\",\n",
    " \"Beat\": 833,\n",
    " \"Ward\": 23\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa94d1a-fedf-44e0-88bf-5b9c88807f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycurl in /opt/conda/lib/python3.11/site-packages (7.45.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a60f86-639d-4952-8eaa-459f356c7ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: curl: command not found\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    " 'http://localhost:8000/predict/' \\\n",
    " -H 'accept: application/json' \\\n",
    " -H 'Content-Type: application/json' \\\n",
    " -d '{ \"Date\": \"08/31/2023 07:00:00 PM\", \"Block\": \"042XX W MARQUETTE RD\", \"IUCR\": \"0498\", \"Location Description\": \"APARTMENT\", \"Arrest\": \"Y\", \"Domestic\": \"Y\", \"Beat\": 833, \"Ward\": 23 }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb02f06-7a1a-4178-bc0e-86320c042eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': [{'type': 'missing', 'loc': ['body', 'Location'], 'msg': 'Field required', 'input': {'Date': '08/31/2023 07:00:00 PM', 'Block': '042XX W MARQUETTE RD', 'IUCR': '0498', 'Location Description': 'APARTMENT', 'Arrest': 'true', 'Domestic': 'true', 'Beat': 833, 'Ward': 23}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/predict/'\n",
    "data = {\n",
    "    \"Date\": \"08/31/2023 07:00:00 PM\",\n",
    "    \"Block\": \"042XX W MARQUETTE RD\",\n",
    "    \"IUCR\": \"0498\",\n",
    "    \"Location Description\": \"APARTMENT\",\n",
    "    \"Arrest\": \"true\",\n",
    "    \"Domestic\": \"true\",\n",
    "    \"Beat\": 833,\n",
    "    \"Ward\": 23\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2b7252-34a1-49cf-8c63-d0f96cfd3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: curl: command not found\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST 'http://localhost:8000/predict/' \\\n",
    " -H 'accept: application/json' \\\n",
    " -H 'Content-Type: application/json' \\\n",
    " -d '{\"Date\": \"08/31/2023 07:00:00 PM\", \"Block\": \"042XX W MARQUETTE RD\", \"IUCR\": \"0498\", \"Location Description\": \"APARTMENT\", \"Arrest\": \"Y\", \"Domestic\": \"Y\", \"Beat\": 833, \"Ward\": 23}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011a17d6-ceef-4e6d-89f3-1cf960207c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': [{'type': 'missing', 'loc': ['body', 'Location'], 'msg': 'Field required', 'input': {'Date': '08/31/2023 07:00:00 PM', 'Block': '042XX W MARQUETTE RD', 'IUCR': '0498', 'Location Description': 'APARTMENT', 'Arrest': 'Y', 'Domestic': 'Y', 'Beat': 833, 'Ward': 23}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = \"http://127.0.0.1:8000/predict/\"\n",
    "\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"08/31/2023 07:00:00 PM\",\n",
    "    \"Block\": \"042XX W MARQUETTE RD\",\n",
    "    \"IUCR\": \"0498\",\n",
    "    \"Location Description\": \"APARTMENT\",\n",
    "    \"Arrest\": \"Y\",\n",
    "    \"Domestic\": \"Y\",\n",
    "    \"Beat\": 833,\n",
    "    \"Ward\": 23\n",
    "}\n",
    "\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Imprimir la respuesta de la API\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe48d621-8ef7-4560-bf3c-14f61e510303",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1284938122.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    Primary Type: str\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "   Date: str\n",
    "   Block: str\n",
    "   IUCR: str\n",
    "   Location: str\n",
    "   Arrest: str\n",
    "   Domestic: str\n",
    "   Beat: int\n",
    "   Ward: int\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "   Primary Type: str\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/', response_model=CrimePredictionOutput)\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "   # Convertir los datos de entrada en un DataFrame\n",
    "   input_data = pd.DataFrame([data.dict()])\n",
    "   # Preprocesar los datos de entrada\n",
    "   input_data = pd.get_dummies(input_data)\n",
    "   # Simulación de las columnas X del entrenamiento para obtener required_columns\n",
    "   X = pd.DataFrame(columns=['Date', 'Block', 'IUCR', 'Location', 'Arrest', 'Domestic', 'Beat', 'Ward'])\n",
    "   required_columns = set(X.columns)\n",
    "   missing_columns = required_columns - set(input_data.columns)\n",
    "   for column in missing_columns:\n",
    "       input_data[column] = 0\n",
    "   # Asegurarse de que las columnas estén en el orden correcto\n",
    "   input_data = input_data[X.columns]\n",
    "   # Hacer la predicción\n",
    "   prediction = model.predict(input_data)\n",
    "   # Obtener la descripción primaria del crimen predicho\n",
    "   Description = prediction[0]\n",
    "   return CrimePredictionOutput(Primary Type=Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5705c989-803f-4e0e-a17e-caa2942e9354",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 31 (3306896995.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 32\u001b[0;36m\u001b[0m\n\u001b[0;31m    input_data[column] = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 31\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    " Date: str\n",
    " Block: str\n",
    " IUCR: str\n",
    " Location: str\n",
    " Arrest: str\n",
    " Domestic: str\n",
    " Beat: int\n",
    " Ward: int\n",
    "class CrimePredictionOutput(BaseModel):\n",
    " PRIMARY_DESCRIPTION: str\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    " # Convertir los datos de entrada en un DataFrame\n",
    " input_data = pd.DataFrame([data.dict()])\n",
    " # Preprocesar los datos de entrada\n",
    " input_data = pd.get_dummies(input_data)\n",
    " # Asegurarse de que todas las columnas necesarias estén presentes\n",
    " required_columns = set(X.columns)\n",
    " missing_columns = required_columns - set(input_data.columns)\n",
    " for column in missing_columns:\n",
    " input_data[column] = 0\n",
    " # Hacer la predicción\n",
    " prediction = model.predict(input_data)\n",
    " # Obtener la descripción primaria del crimen predicho\n",
    " primary_description = prediction[0]\n",
    " return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c3ac03-8578-4a71-a071-4c3f21a88a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: str\n",
    "    Domestic: str\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    # Convertir los datos de entrada en un DataFrame\n",
    "    input_data = pd.DataFrame([data.dict()])\n",
    "    \n",
    "    # Preprocesar los datos de entrada\n",
    "    input_data = pd.get_dummies(input_data)\n",
    "    \n",
    "    # Asegurarse de que todas las columnas necesarias estén presentes\n",
    "    required_columns = set(X.columns)\n",
    "    missing_columns = required_columns - set(input_data.columns)\n",
    "    for column in missing_columns:\n",
    "        input_data[column] = 0\n",
    "    \n",
    "    # Hacer la predicción\n",
    "    prediction = model.predict(input_data)\n",
    "    \n",
    "    # Obtener la descripción primaria del crimen predicho\n",
    "    primary_description = prediction[0]\n",
    "    return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c884669-97f3-4432-baf5-dcd17f38fd01",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Imprimir la respuesta de la API\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"22/10/2024 12:00:00 AM\",\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": \"false\",\n",
    "    \"Domestic\": \"false\",\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31\n",
    "}\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "# Imprimir la respuesta de la API\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d49f7f43-728b-493a-b913-f4e7514af110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 422\n",
      "Response Text: {\"detail\":[{\"type\":\"string_type\",\"loc\":[\"body\",\"Arrest\"],\"msg\":\"Input should be a valid string\",\"input\":false},{\"type\":\"string_type\",\"loc\":[\"body\",\"Domestic\"],\"msg\":\"Input should be a valid string\",\"input\":false}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"22/10/2024 12:00:00 AM\",\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": False,\n",
    "    \"Domestic\": False,\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31,\n",
    "    \"Updated On\": \"22/10/2024 12:00:00 AM\",\n",
    "    \"Year\": 2024,\n",
    "    \"ID\": 123456789,\n",
    "    \"Location Description\": \"APARTMENT\",\n",
    "    \"Description\": \"THEFT\",\n",
    "    \"District\": 25,\n",
    "    \"Community Area\": 24\n",
    "}\n",
    "\n",
    "\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Imprimir el contenido de la respuesta\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1a04fa3-e1ee-4a5e-88c7-4c1b6f1ae550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Cargar el conjunto de datos original para obtener las columnas necesarias\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar las características (X) y la etiqueta (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Definir el preprocesador para codificación de columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador con los datos originales\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: str\n",
    "    Domestic: str\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    # Convertir los datos de entrada en un DataFrame\n",
    "    input_data = pd.DataFrame([data.dict()])\n",
    "\n",
    "    # Preprocesar los datos de entrada\n",
    "    input_data_transformed = preprocessor.transform(input_data)\n",
    "\n",
    "    # Hacer la predicción\n",
    "    prediction = model.predict(input_data_transformed)\n",
    "\n",
    "    # Obtener la descripción primaria del crimen predicho\n",
    "    primary_description = prediction[0]\n",
    "    return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7bb296d-c27b-4d02-a93e-1bfefa13baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Cargar el conjunto de datos original para obtener las columnas necesarias\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar las características (X) y la etiqueta (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Definir el preprocesador para codificación de columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador con los datos originales\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: str\n",
    "    Domestic: str\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    # Convertir los datos de entrada en un DataFrame\n",
    "    input_data = pd.DataFrame([data.dict()])\n",
    "\n",
    "    # Asegurarse de que todas las columnas necesarias estén presentes\n",
    "    required_columns = set(X.columns)\n",
    "    missing_columns = required_columns - set(input_data.columns)\n",
    "    for column in missing_columns:\n",
    "        input_data[column] = 0\n",
    "    \n",
    "    # Preprocesar los datos de entrada\n",
    "    input_data_transformed = preprocessor.transform(input_data)\n",
    "\n",
    "    # Hacer la predicción\n",
    "    prediction = model.predict(input_data_transformed)\n",
    "\n",
    "    # Obtener la descripción primaria del crimen predicho\n",
    "    primary_description = prediction[0]\n",
    "    return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad92118-7866-43c9-97da-2be226c162a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"22/10/2024 12:00:00 AM\",\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": False,\n",
    "    \"Domestic\": False,\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31\n",
    "}\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "# Imprimir la respuesta de la API\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19ac4a53-45f0-4a7e-b5a6-e2b73ddf5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Cargar el conjunto de datos original para obtener las columnas necesarias\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar las características (X) y la etiqueta (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Definir el preprocesador para codificación de columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador con los datos originales\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: bool\n",
    "    Domestic: bool\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    # Convertir los datos de entrada en un DataFrame\n",
    "    input_data = pd.DataFrame([data.dict()])\n",
    "\n",
    "    # Asegurarse de que Arrest y Domestic sean booleanos\n",
    "    input_data['Arrest'] = input_data['Arrest'].astype(bool)\n",
    "    input_data['Domestic'] = input_data['Domestic'].astype(bool)\n",
    "\n",
    "    # Convertir Date al formato correcto si es necesario\n",
    "    try:\n",
    "        input_data['Date'] = pd.to_datetime(input_data['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "    except ValueError:\n",
    "        raise HTTPException(status_code=400, detail=\"Date format should be 'MM/DD/YYYY HH:MM:SS AM/PM'\")\n",
    "\n",
    "    # Asegurarse de que todas las columnas necesarias estén presentes\n",
    "    required_columns = set(X.columns)\n",
    "    missing_columns = required_columns - set(input_data.columns)\n",
    "    for column in missing_columns:\n",
    "        input_data[column] = 0\n",
    "    \n",
    "    # Preprocesar los datos de entrada\n",
    "    input_data_transformed = preprocessor.transform(input_data)\n",
    "\n",
    "    # Hacer la predicción\n",
    "    prediction = model.predict(input_data_transformed)\n",
    "\n",
    "    # Obtener la descripción primaria del crimen predicho\n",
    "    primary_description = prediction[0]\n",
    "    return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cb8cc1b-e2d8-4bfa-a0ae-8731b29316f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.994202552785399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['crime_prediction_model.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "# Paso 2: Preprocesamiento de datos\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location Description'])\n",
    "# Codificar las variables categóricas con LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "   le = LabelEncoder()\n",
    "   data[column] = le.fit_transform(data[column])\n",
    "   label_encoders[column] = le\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "y = data['Primary Type']\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Paso 4: Evaluación del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Paso 5: Guardar el modelo entrenado\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44abd582-c9b5-4f3d-be32-e3d0e448f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Cargar el conjunto de datos original para obtener las columnas necesarias\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location Description'])\n",
    "\n",
    "# Separar las características (X) y la etiqueta (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Definir el preprocesador para codificación de columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador con los datos originales\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: bool\n",
    "    Domestic: bool\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    # Convertir los datos de entrada en un DataFrame\n",
    "    input_data = pd.DataFrame([data.dict()])\n",
    "\n",
    "    # Asegurarse de que Arrest y Domestic sean booleanos\n",
    "    input_data['Arrest'] = input_data['Arrest'].astype(bool)\n",
    "    input_data['Domestic'] = input_data['Domestic'].astype(bool)\n",
    "\n",
    "    # Convertir Date al formato correcto si es necesario\n",
    "    try:\n",
    "        input_data['Date'] = pd.to_datetime(input_data['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "    except ValueError:\n",
    "        raise HTTPException(status_code=400, detail=\"Date format should be 'MM/DD/YYYY HH:MM:SS AM/PM'\")\n",
    "\n",
    "    # Asegurarse de que todas las columnas necesarias estén presentes\n",
    "    required_columns = set(X.columns)\n",
    "    missing_columns = required_columns - set(input_data.columns)\n",
    "    for column in missing_columns:\n",
    "        input_data[column] = 0\n",
    "    \n",
    "    # Preprocesar los datos de entrada\n",
    "    input_data_transformed = preprocessor.transform(input_data)\n",
    "\n",
    "    # Hacer la predicción\n",
    "    prediction = model.predict(input_data_transformed)\n",
    "\n",
    "    # Obtener la descripción primaria del crimen predicho\n",
    "    primary_description = prediction[0]\n",
    "    return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a96b1e1e-84e4-4fe8-bab0-9db84153e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': \"Date format should be 'MM/DD/YYYY HH:MM:SS AM/PM'\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"22/10/2024 12:00:00 AM\",\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": False,\n",
    "    \"Domestic\": False,\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31\n",
    "}\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "# Imprimir la respuesta de la API\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0fe7280-9996-42ab-a04d-bc11d4b47998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Response Text: Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"10/22/2024 12:00:00 AM\",  # Formato MM/DD/YYYY HH:MM:SS AM/PM\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": False,\n",
    "    \"Domestic\": False,\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31\n",
    "}\n",
    "\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Imprimir el contenido de la respuesta\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84395589-84d5-4a20-9622-818880367787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Response Text: Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"10/22/2024 12:00:00 AM\",  # Formato MM/DD/YYYY HH:MM:SS AM/PM\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": True,  # Booleanos\n",
    "    \"Domestic\": True,  # Booleanos\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31\n",
    "}\n",
    "\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Imprimir el contenido de la respuesta\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68ccd35b-d0a2-4f47-ac80-7f53382386cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Cargar el conjunto de datos original para obtener las columnas necesarias\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar las características (X) y la etiqueta (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Definir el preprocesador para codificación de columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador con los datos originales\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: bool\n",
    "    Domestic: bool\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    try:\n",
    "        # Convertir los datos de entrada en un DataFrame\n",
    "        input_data = pd.DataFrame([data.dict()])\n",
    "\n",
    "        # Asegurarse de que Arrest y Domestic sean booleanos\n",
    "        input_data['Arrest'] = input_data['Arrest'].astype(bool)\n",
    "        input_data['Domestic'] = input_data['Domestic'].astype(bool)\n",
    "\n",
    "        # Convertir Date al formato correcto\n",
    "        input_data['Date'] = pd.to_datetime(input_data['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "        # Asegurarse de que todas las columnas necesarias estén presentes\n",
    "        required_columns = set(X.columns)\n",
    "        missing_columns = required_columns - set(input_data.columns)\n",
    "        for column in missing_columns:\n",
    "            input_data[column] = 0\n",
    "        \n",
    "        # Preprocesar los datos de entrada\n",
    "        input_data_transformed = preprocessor.transform(input_data)\n",
    "\n",
    "        # Hacer la predicción\n",
    "        prediction = model.predict(input_data_transformed)\n",
    "\n",
    "        # Obtener la descripción primaria del crimen predicho\n",
    "        primary_description = prediction[0]\n",
    "        return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47c88e0f-78fd-4df1-834f-14c70cd9ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Response Text: {\"detail\":\"ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "\n",
    "# Datos JSON para la solicitud POST\n",
    "data = {\n",
    "    \"Date\": \"10/22/2024 12:00:00 AM\",  # Formato MM/DD/YYYY HH:MM:SS AM/PM\n",
    "    \"Block\": \"041XX W OAKDALE AVE\",\n",
    "    \"IUCR\": \"1153\",\n",
    "    \"Location\": \"POINT (-87.730709826 41.934480986)\",\n",
    "    \"Arrest\": True,  # Booleanos\n",
    "    \"Domestic\": True,  # Booleanos\n",
    "    \"Beat\": 2523,\n",
    "    \"Ward\": 31\n",
    "}\n",
    "\n",
    "# Headers de la solicitud\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Enviar la solicitud POST y obtener la respuesta\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Imprimir el contenido de la respuesta\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e94d96f-633b-4381-b0da-8be20349f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Paso 1: Cargar el modelo entrenado\n",
    "model = joblib.load('crime_prediction_model.pkl')\n",
    "\n",
    "# Cargar el conjunto de datos original para obtener las columnas necesarias\n",
    "data = pd.read_csv('Crimes_-_2024_20241031.csv')\n",
    "\n",
    "# Eliminar columnas innecesarias o que no sean útiles para la predicción\n",
    "data = data.drop(columns=['Case Number', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location'])\n",
    "\n",
    "# Separar las características (X) y la etiqueta (y)\n",
    "X = data.drop(columns=['Primary Type'])\n",
    "\n",
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Definir el preprocesador para codificación de columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador con los datos originales\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Paso 2: Definir los modelos de datos para la entrada y salida de la API\n",
    "class CrimePredictionInput(BaseModel):\n",
    "    Date: str\n",
    "    Block: str\n",
    "    IUCR: str\n",
    "    Location: str\n",
    "    Arrest: bool\n",
    "    Domestic: bool\n",
    "    Beat: int\n",
    "    Ward: int\n",
    "\n",
    "class CrimePredictionOutput(BaseModel):\n",
    "    PRIMARY_DESCRIPTION: str\n",
    "\n",
    "# Paso 3: Inicializar la aplicación FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Paso 4: Definir el endpoint para hacer predicciones de crimen\n",
    "@app.post('/predict/')\n",
    "async def predict_crime(data: CrimePredictionInput):\n",
    "    try:\n",
    "        # Convertir los datos de entrada en un DataFrame\n",
    "        input_data = pd.DataFrame([data.dict()])\n",
    "\n",
    "        # Asegurarse de que Arrest y Domestic sean booleanos\n",
    "        input_data['Arrest'] = input_data['Arrest'].astype(bool)\n",
    "        input_data['Domestic'] = input_data['Domestic'].astype(bool)\n",
    "\n",
    "        # Convertir Date al formato correcto\n",
    "        input_data['Date'] = pd.to_datetime(input_data['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "        # Asegurarse de que todas las columnas necesarias estén presentes\n",
    "        required_columns = set(X.columns)\n",
    "        missing_columns = required_columns - set(input_data.columns)\n",
    "        for column in missing_columns:\n",
    "            input_data[column] = 0\n",
    "\n",
    "        # Verificar y manejar valores nulos\n",
    "        input_data.fillna(0, inplace=True)\n",
    "\n",
    "        # Preprocesar los datos de entrada\n",
    "        input_data_transformed = preprocessor.transform(input_data)\n",
    "\n",
    "        # Verificar y manejar valores nulos en los datos transformados\n",
    "        input_data_transformed = np.nan_to_num(input_data_transformed)\n",
    "\n",
    "        # Hacer la predicción\n",
    "        prediction = model.predict(input_data_transformed)\n",
    "\n",
    "        # Obtener la descripción primaria del crimen predicho\n",
    "        primary_description = prediction[0]\n",
    "        return CrimePredictionOutput(PRIMARY_DESCRIPTION=primary_description)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b1002-3503-41e1-a3bf-583cc0de007f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
